{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# make sure to install these packages before running:\n",
    "# pip install pandas\n",
    "# pip install sodapy\n",
    "# pip install google-cloud-storage\n",
    "# pip install google-cloud-bigquery\n",
    "# pip install pandas-gbq\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from sodapy import Socrata\n",
    "import pandas_gbq as gbq\n",
    "from datetime import datetime\n",
    "from gcloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is needed for local development in order to appropriately access GCP from a local service account key\n",
    "#https://cloud.google.com/docs/authentication/production#auth-cloud-compute-engine-python\n",
    "#may not use this at all and have cloud functions use the default service account\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"./service_account.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define environment variables\n",
    "project_id = 'iconic-range-220603' #capture the project id to where this data will land\n",
    "bucket_name = 'test_sung_1' #capture bucket name where raw data will be stored\n",
    "dataset_name = 'test_sung_1' #initial dataset\n",
    "table_name = 'test_sung_5' #name of table to capture data\n",
    "table_desc = 'Raw, public Chicago traffic data is appended to this table every 5 minutes'#table description\n",
    "nulls_expected = ('_comments') #tuple of nulls expected for checking data outliers\n",
    "partition_by = '_last_updt' #partition by the last updated field for faster querying and incremental loadsl\n",
    "schema_bq = [\n",
    "    bigquery.SchemaField('_comments', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_direction', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_fromst', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_last_updt', 'TIMESTAMP', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_length', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_lif_lat', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_lit_lat', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_lit_lon', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_strheading', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_tost', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_traffic', 'INTEGER', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('segmentid', 'INTEGER', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('start_lon', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('street', 'STRING', mode='NULLABLE')\n",
    "] #apply a schema during BigQuery table creation\n",
    "\n",
    "schema_df = {'_comments': 'object', \n",
    "               '_direction': 'object', \n",
    "               '_fromst': 'object', \n",
    "               '_last_updt': 'datetime64',          \n",
    "               '_length': 'float64', \n",
    "               '_lif_lat': 'float64', \n",
    "               '_lit_lat': 'float64', \n",
    "               '_lit_lon': 'float64',\n",
    "               '_strheading': 'object', \n",
    "               '_tost': 'object', \n",
    "               '_traffic': 'int64', \n",
    "               'segmentid': 'int64', \n",
    "               'start_lon': 'float64', \n",
    "               'street': 'object'\n",
    "              } #apply a schema to pandas dataframe to match BigQuery for equivalent data types\n",
    "\n",
    "#may want to define bigquery client, dataset_ref, and table_ref earlier in the handler function to avoid redundant code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getToday():\n",
    "    \"\"\"Create timestamp string\"\"\"\n",
    "    return datetime.now().strftime('%Y%m%d%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = _getToday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name):\n",
    "    \"\"\"Detects whether or not a new bucket needs to be created\"\"\"\n",
    "    client = storage.Client.from_service_account_json('service_account.json') #authenticate service account\n",
    "    bucket = client.bucket(bucket_name) #capture bucket details\n",
    "    bucket.location = 'US-CENTRAL1' #define regional location\n",
    "    if not bucket.exists(): #checks if bucket doesn't exist\n",
    "        bucket.create()\n",
    "        print(\"Created a new bucket: {0}\".format(bucket_name))\n",
    "    else:\n",
    "        print(\"Bucket already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a new bucket: test_sung_1\n"
     ]
    }
   ],
   "source": [
    "create_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_exists(client, dataset_reference):\n",
    "    \"\"\"Return if a table exists.\n",
    "\n",
    "    Args:\n",
    "        client (google.cloud.bigquery.client.Client):\n",
    "            A client to connect to the BigQuery API.\n",
    "        table_reference (google.cloud.bigquery.table.TableReference):\n",
    "            A reference to the table to look for.\n",
    "\n",
    "    Returns:\n",
    "        bool: ``True`` if the table exists, ``False`` otherwise.\n",
    "    \"\"\"\n",
    "    from google.cloud.exceptions import NotFound\n",
    "\n",
    "    try:\n",
    "        client.get_dataset(dataset_reference)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_exists(client, table_reference):\n",
    "    \"\"\"Return if a table exists.\n",
    "\n",
    "    Args:\n",
    "        client (google.cloud.bigquery.client.Client):\n",
    "            A client to connect to the BigQuery API.\n",
    "        table_reference (google.cloud.bigquery.table.TableReference):\n",
    "            A reference to the table to look for.\n",
    "\n",
    "    Returns:\n",
    "        bool: ``True`` if the table exists, ``False`` otherwise.\n",
    "    \"\"\"\n",
    "    from google.cloud.exceptions import NotFound\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_reference)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://cloud.google.com/bigquery/docs/python-client-migration#update_a_table\n",
    "def create_dataset_table(dataset_name, table_name, table_desc, schema, partition_by):\n",
    "    \"\"\"Detects whether or not a new dataset and/or table need to be created\"\"\"\n",
    "    #setup the client\n",
    "    bigquery_client = bigquery.Client()\n",
    "\n",
    "    # Create a DatasetReference using a chosen dataset ID.\n",
    "    dataset_ref = bigquery_client.dataset(dataset_name) # The project defaults to the Client's project if not specified.\n",
    "\n",
    "    # Construct a full Dataset object to send to the API.\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "\n",
    "    # Specify the geographic location where the dataset should reside.\n",
    "    dataset.location = \"US\"\n",
    "    \n",
    "    # Send the dataset to the API for creation.\n",
    "    # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "    # exists within the project.\n",
    "    if dataset_exists(bigquery_client, dataset_ref) == False: #checks if dataset not found\n",
    "        dataset = bigquery_client.create_dataset(dataset)  # API request\n",
    "        print(\"Created new dataset\")\n",
    "    else:\n",
    "        print(\"Dataset already exists\")\n",
    "    \n",
    "    #Create an empty table\n",
    "    table_ref = dataset_ref.table(table_name) #construct a full table object to send to the api\n",
    "    \n",
    "    if table_exists(bigquery_client, table_ref) == False: #checks if table not found\n",
    "        table = bigquery.Table(table_ref, schema=schema)\n",
    "        table.time_partitioning = bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY, #day is the only supported type for now\n",
    "            field=partition_by)  # name of column to use for partitioning\n",
    "        table = bigquery_client.create_table(table)  # API request\n",
    "        assert table.table_id == table_name #checks if table_id matches table_name\n",
    "        \n",
    "        #update the table description\n",
    "        table.description = table_desc\n",
    "        table = bigquery_client.update_table(table, ['description'])  # API request\n",
    "        assert table.description == table_desc #checks if table description matches the update\n",
    "        print(\"Created empty table partitioned on column: {}\".format(table.time_partitioning.field))\n",
    "    else:\n",
    "        print(\"Table already exists within dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists\n",
      "Table already exists within dataset\n"
     ]
    }
   ],
   "source": [
    "create_dataset_table(dataset_name, table_name, table_desc, schema_bq, partition_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pandas dataframe\n",
    "def create_results_df():\n",
    "    \"\"\"Create a dataframe based on JSON from the Chicago traffic API\"\"\"\n",
    "    try:\n",
    "        # First 2000 results, returned as JSON from API / converted to Python list of\n",
    "        # dictionaries by sodapy.\n",
    "        # Unauthenticated client only works with public data sets. Note 'None'\n",
    "        # in place of application token, and no username or password:\n",
    "        data_client = Socrata(\"data.cityofchicago.org\", None)\n",
    "        results = data_client.get(\"8v9j-bter\", limit=2000) #unique id for chicago traffic data\n",
    "\n",
    "        # Convert to pandas DataFrame\n",
    "        results_df = pd.DataFrame.from_records(results)\n",
    "        print(\"Successfully created a pandas dataframe!\")\n",
    "        \n",
    "        return results_df\n",
    "    except Exception as e:\n",
    "        print(\"Failure to create a pandas dataframe :(\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created a pandas dataframe!\n"
     ]
    }
   ],
   "source": [
    "results_df = create_results_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function upload data to cloud storage\n",
    "def upload_raw_data_gcs(results_df, bucket_name):\n",
    "    \"\"\"Upload dataframe into google cloud storage bucket\"\"\"\n",
    "    try: \n",
    "        # Write the DataFrame to GCS (Google Cloud Storage)\n",
    "        storage_client = storage.Client.from_service_account_json('service_account.json') #authenticate service account\n",
    "        bucket = storage_client.bucket(bucket_name) #capture bucket details\n",
    "        results_df.to_csv('traffic_' + _getToday() + '.csv') #convert dataframe to csv file type\n",
    "        source_file_name = 'traffic_' + _getToday() + '.csv' #create the file name\n",
    "        blob = bucket.blob(os.path.basename(source_file_name)) #define the path to the file\n",
    "        blob.upload_from_filename(source_file_name) #upload to bucket\n",
    "        print(\"Successfully uploaded csv file into {}\".format(bucket))\n",
    "    except Exception as e:\n",
    "        print(\"Failure to upload data to google cloud storage bucket :(\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded csv file into <Bucket: test_sung_1>\n"
     ]
    }
   ],
   "source": [
    "upload_raw_data_gcs(results_df, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/21886742/convert-pandas-dtypes-to-bigquery-type-representation\n",
    "#https://stackoverflow.com/questions/44953463/pandas-google-bigquery-schema-mismatch-makes-the-upload-fail\n",
    "#http://pbpython.com/pandas_dtypes.html\n",
    "    \n",
    "def convert_schema(results_df, schema_dict):\n",
    "    \"\"\"Converts data types in dataframe to match BigQuery destination table\"\"\"\n",
    "    for k, v in schema_dict.items(): #for each column name in the dictionary, convert the data type in the dataframe\n",
    "        results_df[k] = results_df[k].astype(v)\n",
    "    results_df_transformed = results_df\n",
    "    print(\"Updated schema to match BigQuery destination table\")\n",
    "    return results_df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated schema to match BigQuery destination table\n",
      "_comments              object\n",
      "_direction             object\n",
      "_fromst                object\n",
      "_last_updt     datetime64[ns]\n",
      "_length               float64\n",
      "_lif_lat              float64\n",
      "_lit_lat              float64\n",
      "_lit_lon              float64\n",
      "_strheading            object\n",
      "_tost                  object\n",
      "_traffic                int64\n",
      "segmentid               int64\n",
      "start_lon             float64\n",
      "street                 object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "results_df_transformed = convert_schema(results_df, schema_df)\n",
    "\n",
    "print(results_df_transformed.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_comments       True\n",
       "_direction     False\n",
       "_fromst        False\n",
       "_last_updt     False\n",
       "_length        False\n",
       "_lif_lat       False\n",
       "_lit_lat       False\n",
       "_lit_lon       False\n",
       "_strheading    False\n",
       "_tost          False\n",
       "_traffic       False\n",
       "segmentid      False\n",
       "start_lon      False\n",
       "street         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = results_df_transformed.isnull().any()\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are any nulls in the columns except for _comments\n",
    "\n",
    "def check_nulls(results_df_transformed):\n",
    "    \"\"\"Checks if there are any nulls in the columns\"\"\"\n",
    "    try:\n",
    "        null_columns = []\n",
    "        check_bool = results_df_transformed.isnull().any() #returns a boolean True/False for every column in dataframe if it contains nulls\n",
    "        for k, v in check_bool.items(): #for each column in check_bool having nulls, print the name of the column\n",
    "            if check_bool[v] == False:\n",
    "                null_columns.append(k)\n",
    "        print(\"These are the null columns: {}\".format(null_columns))\n",
    "        return null_columns\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the null columns: ['_comments']\n"
     ]
    }
   ],
   "source": [
    "null_columns = check_nulls(results_df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_outliers(null_columns, nulls_expected):\n",
    "    \"\"\"Checks if there are any outlier nulls in the columns\"\"\"\n",
    "    try:\n",
    "        null_outliers=[] #empty list to collect list of columns that are not expected to be null\n",
    "        #if any columns in the nulls expected mismatch the nulls collected, append to null_outliers\n",
    "        if any(x not in nulls_expected for x in null_columns): \n",
    "            null_outliers.append(x)\n",
    "        print(\"These are the outlier null columns: {}\".format(null_outliers))\n",
    "        return null_outliers\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the outlier null columns: []\n"
     ]
    }
   ],
   "source": [
    "null_outliers = check_null_outliers(null_columns, nulls_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out the nullable vs. required mode schema mismatch\n",
    "def upload_to_gbq(results_df_transformed, project_id, dataset_name, table_name):\n",
    "    \"\"\"Uploads data into bigquery and appends if data already exists\"\"\"\n",
    "    try:\n",
    "        gbq.to_gbq(results_df_transformed, dataset_name+\".\"+table_name, project_id, \n",
    "                   if_exists='append', location = 'US', progress_bar=False)\n",
    "        print('Data uploaded into project: {0}, dataset: {1}, table: {2}'.format(project_id, dataset_name, table_name))\n",
    "    except Exception as e:\n",
    "        print(\"Failure to upload data to Bigquery :(\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded into project: iconic-range-220603, dataset: test_sung_1, table: test_sung_5\n"
     ]
    }
   ],
   "source": [
    "upload_to_gbq(results_df_transformed, project_id, dataset_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get record count in raw table\n",
    "# from google.cloud import bigquery\n",
    "# client = bigquery.Client()\n",
    "# dataset_id = 'my_dataset'\n",
    "# table_id = 'my_table'\n",
    "def bq_table_num_rows(dataset_name, table_name):\n",
    "    \"\"\"Print total number of rows in destination bigquery table\"\"\"\n",
    "    try:\n",
    "        bigquery_client = bigquery.Client() #instantiate bigquery client to interact with api\n",
    "        dataset_ref = bigquery_client.dataset(dataset_name) #create dataset object\n",
    "        table_ref = dataset_ref.table(table_name) #create table object \n",
    "        table = bigquery_client.get_table(table_ref)  # API Request\n",
    "        num_rows = table.num_rows\n",
    "        print(\"Total number of rows: {0} in dataset: {1}, table:{2}\".format(table.num_rows, dataset_name, table_name))\n",
    "        return num_rows\n",
    "    except Exception as e:\n",
    "        print(\"Failure to count number of rows in dataset: {0}, table: {1}\".format(dataset_name, table_name))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 1257 in dataset: test_sung_1, table:test_sung_5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1257"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq_table_num_rows(dataset_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return f'Uploaded raw csv file to bucket: {0}, and appended data to BigQuery dataset: {1}, table: {2} on '.format(bucket_name,dataset_name,table_name) + _getToday()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
