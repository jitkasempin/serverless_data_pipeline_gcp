{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# make sure to install these packages before running:\n",
    "# pip install pandas\n",
    "# pip install sodapy\n",
    "# pip install google-cloud-storage\n",
    "# pip install google-cloud-bigquery\n",
    "# pip install pandas-gbq\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from sodapy import Socrata\n",
    "import pandas_gbq as gbq\n",
    "from datetime import datetime\n",
    "from gcloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is needed for local development in order to appropriately access GCP from a local service account key\n",
    "#https://cloud.google.com/docs/authentication/production#auth-cloud-compute-engine-python\n",
    "#may not use this at all and have cloud functions use the default service account\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"./service_account.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define environment variables\n",
    "project_id = 'iconic-range-220603' #capture the project id to where this data will land\n",
    "bucket_name = 'test_sung_1' #capture bucket name where raw data will be stored\n",
    "dataset_name = 'test_sung_1' #initial dataset\n",
    "table_name = 'test_sung_5' #name of table to capture data\n",
    "table_desc = 'Raw, public Chicago traffic data is appended to this table every 5 minutes'#table description\n",
    "nulls_expected = ('_comments') #tuple of nulls expected for checking data outliers\n",
    "partition_by = '_last_updt' #partition by the last updated field for faster querying and incremental loadsl\n",
    "schema_bq = [\n",
    "    bigquery.SchemaField('_comments', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_direction', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_fromst', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_last_updt', 'TIMESTAMP', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_length', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_lif_lat', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_lit_lat', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_lit_lon', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_strheading', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_tost', 'STRING', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('_traffic', 'INTEGER', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('segmentid', 'INTEGER', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('start_lon', 'FLOAT', mode='NULLABLE'),\n",
    "    bigquery.SchemaField('street', 'STRING', mode='NULLABLE')\n",
    "] #apply a schema during BigQuery table creation\n",
    "\n",
    "schema_df = {'_comments': 'object', \n",
    "               '_direction': 'object', \n",
    "               '_fromst': 'object', \n",
    "               '_last_updt': 'datetime64',          \n",
    "               '_length': 'float64', \n",
    "               '_lif_lat': 'float64', \n",
    "               '_lit_lat': 'float64', \n",
    "               '_lit_lon': 'float64',\n",
    "               '_strheading': 'object', \n",
    "               '_tost': 'object', \n",
    "               '_traffic': 'int64', \n",
    "               'segmentid': 'int64', \n",
    "               'start_lon': 'float64', \n",
    "               'street': 'object'\n",
    "              } #apply a schema to pandas dataframe to match BigQuery for equivalent data types\n",
    "\n",
    "#may want to define bigquery client, dataset_ref, and table_ref earlier in the handler function to avoid redundant code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getToday():\n",
    "    \"\"\"Create timestamp string\"\"\"\n",
    "    return datetime.now().strftime('%Y%m%d%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = _getToday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name):\n",
    "    \"\"\"Detects whether or not a new bucket needs to be created\"\"\"\n",
    "    client = storage.Client.from_service_account_json('service_account.json') #authenticate service account\n",
    "    bucket = client.bucket(bucket_name) #capture bucket details\n",
    "    bucket.location = 'US-CENTRAL1' #define regional location\n",
    "    if not bucket.exists(): #checks if bucket doesn't exist\n",
    "        bucket.create()\n",
    "        print(\"Created a new bucket: {0}\".format(bucket_name))\n",
    "    else:\n",
    "        print(\"Bucket already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket already exists\n"
     ]
    }
   ],
   "source": [
    "create_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_exists(client, dataset_reference):\n",
    "    \"\"\"Return if a table exists.\n",
    "\n",
    "    Args:\n",
    "        client (google.cloud.bigquery.client.Client):\n",
    "            A client to connect to the BigQuery API.\n",
    "        table_reference (google.cloud.bigquery.table.TableReference):\n",
    "            A reference to the table to look for.\n",
    "\n",
    "    Returns:\n",
    "        bool: ``True`` if the table exists, ``False`` otherwise.\n",
    "    \"\"\"\n",
    "    from google.cloud.exceptions import NotFound\n",
    "\n",
    "    try:\n",
    "        client.get_dataset(dataset_reference)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_exists(client, table_reference):\n",
    "    \"\"\"Return if a table exists.\n",
    "\n",
    "    Args:\n",
    "        client (google.cloud.bigquery.client.Client):\n",
    "            A client to connect to the BigQuery API.\n",
    "        table_reference (google.cloud.bigquery.table.TableReference):\n",
    "            A reference to the table to look for.\n",
    "\n",
    "    Returns:\n",
    "        bool: ``True`` if the table exists, ``False`` otherwise.\n",
    "    \"\"\"\n",
    "    from google.cloud.exceptions import NotFound\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_reference)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://cloud.google.com/bigquery/docs/python-client-migration#update_a_table\n",
    "def create_dataset_table(dataset_name, table_name, table_desc, schema, partition_by):\n",
    "    \"\"\"Detects whether or not a new dataset and/or table need to be created\"\"\"\n",
    "    #setup the client\n",
    "    bigquery_client = bigquery.Client()\n",
    "\n",
    "    # Create a DatasetReference using a chosen dataset ID.\n",
    "    dataset_ref = bigquery_client.dataset(dataset_name) # The project defaults to the Client's project if not specified.\n",
    "\n",
    "    # Construct a full Dataset object to send to the API.\n",
    "    dataset = bigquery.Dataset(dataset_ref)\n",
    "\n",
    "    # Specify the geographic location where the dataset should reside.\n",
    "    dataset.location = \"US\"\n",
    "    \n",
    "    # Send the dataset to the API for creation.\n",
    "    # Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "    # exists within the project.\n",
    "    if dataset_exists(bigquery_client, dataset_ref) == False: #checks if dataset not found\n",
    "        dataset = bigquery_client.create_dataset(dataset)  # API request\n",
    "        print(\"Created new dataset\")\n",
    "    else:\n",
    "        print(\"Dataset already exists\")\n",
    "    \n",
    "    #Create an empty table\n",
    "    table_ref = dataset_ref.table(table_name) #construct a full table object to send to the api\n",
    "    \n",
    "    if table_exists(bigquery_client, table_ref) == False: #checks if table not found\n",
    "        table = bigquery.Table(table_ref, schema=schema)\n",
    "        table.time_partitioning = bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY, #day is the only supported type for now\n",
    "            field=partition_by)  # name of column to use for partitioning\n",
    "        table = bigquery_client.create_table(table)  # API request\n",
    "        assert table.table_id == table_name #checks if table_id matches table_name\n",
    "        \n",
    "        #update the table description\n",
    "        table.description = table_desc\n",
    "        table = bigquery_client.update_table(table, ['description'])  # API request\n",
    "        assert table.description == table_desc #checks if table description matches the update\n",
    "        print(\"Created empty table partitioned on column: {}\".format(table.time_partitioning.field))\n",
    "    else:\n",
    "        print(\"Table already exists within dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists\n",
      "Table already exists within dataset\n"
     ]
    }
   ],
   "source": [
    "create_dataset_table(dataset_name, table_name, table_desc, schema_bq, partition_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pandas dataframe\n",
    "def create_results_df():\n",
    "    \"\"\"Create a dataframe based on JSON from the Chicago traffic API\"\"\"\n",
    "    try:\n",
    "        # First 2000 results, returned as JSON from API / converted to Python list of\n",
    "        # dictionaries by sodapy.\n",
    "        # Unauthenticated client only works with public data sets. Note 'None'\n",
    "        # in place of application token, and no username or password:\n",
    "        data_client = Socrata(\"data.cityofchicago.org\", None)\n",
    "        results = data_client.get(\"8v9j-bter\", limit=2000) #unique id for chicago traffic data\n",
    "\n",
    "        # Convert to pandas DataFrame\n",
    "        results_df = pd.DataFrame.from_records(results)\n",
    "        print(\"Successfully created a pandas dataframe!\")\n",
    "        \n",
    "        return results_df\n",
    "    except Exception as e:\n",
    "        print(\"Failure to create a pandas dataframe :(\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created a pandas dataframe!\n"
     ]
    }
   ],
   "source": [
    "results_df = create_results_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function upload data to cloud storage\n",
    "def upload_raw_data_gcs(results_df, bucket_name):\n",
    "    \"\"\"Upload dataframe into google cloud storage bucket\"\"\"\n",
    "    try: \n",
    "        # Write the DataFrame to GCS (Google Cloud Storage)\n",
    "        storage_client = storage.Client.from_service_account_json('service_account.json') #authenticate service account\n",
    "        bucket = storage_client.bucket(bucket_name) #capture bucket details\n",
    "        results_df.to_csv('traffic_' + _getToday() + '.csv') #convert dataframe to csv file type\n",
    "        source_file_name = 'traffic_' + _getToday() + '.csv' #create the file name\n",
    "        blob = bucket.blob(os.path.basename(source_file_name)) #define the path to the file\n",
    "        blob.upload_from_filename(source_file_name) #upload to bucket\n",
    "        print(\"Successfully uploaded csv file into {}\".format(bucket))\n",
    "    except Exception as e:\n",
    "        print(\"Failure to upload data to google cloud storage bucket :(\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded csv file into <Bucket: test_sung_1>\n"
     ]
    }
   ],
   "source": [
    "upload_raw_data_gcs(results_df, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/21886742/convert-pandas-dtypes-to-bigquery-type-representation\n",
    "#https://stackoverflow.com/questions/44953463/pandas-google-bigquery-schema-mismatch-makes-the-upload-fail\n",
    "#http://pbpython.com/pandas_dtypes.html\n",
    "    \n",
    "def convert_schema(results_df, schema_dict):\n",
    "    \"\"\"Converts data types in dataframe to match BigQuery destination table\"\"\"\n",
    "    for k, v in schema_dict.items(): #for each column name in the dictionary, convert the data type in the dataframe\n",
    "        results_df[k] = results_df[k].astype(v)\n",
    "    results_df_transformed = results_df\n",
    "    print(\"Updated schema to match BigQuery destination table\")\n",
    "    return results_df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated schema to match BigQuery destination table\n",
      "_comments              object\n",
      "_direction             object\n",
      "_fromst                object\n",
      "_last_updt     datetime64[ns]\n",
      "_length               float64\n",
      "_lif_lat              float64\n",
      "_lit_lat              float64\n",
      "_lit_lon              float64\n",
      "_strheading            object\n",
      "_tost                  object\n",
      "_traffic                int64\n",
      "segmentid               int64\n",
      "start_lon             float64\n",
      "street                 object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "results_df_transformed = convert_schema(results_df, schema_df)\n",
    "\n",
    "print(results_df_transformed.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_comments       True\n",
       "_direction     False\n",
       "_fromst        False\n",
       "_last_updt     False\n",
       "_length        False\n",
       "_lif_lat       False\n",
       "_lit_lat       False\n",
       "_lit_lon       False\n",
       "_strheading    False\n",
       "_tost          False\n",
       "_traffic       False\n",
       "segmentid      False\n",
       "start_lon      False\n",
       "street         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = results_df_transformed.isnull().any()\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there are any nulls in the columns except for _comments\n",
    "\n",
    "def check_nulls(results_df_transformed):\n",
    "    \"\"\"Checks if there are any nulls in the columns\"\"\"\n",
    "    try:\n",
    "        null_columns = []\n",
    "        check_bool = results_df_transformed.isnull().any() #returns a boolean True/False for every column in dataframe if it contains nulls\n",
    "        for k, v in check_bool.items(): #for each column in check_bool having nulls, print the name of the column\n",
    "            if check_bool[v] == False:\n",
    "                null_columns.append(k)\n",
    "        print(\"These are the null columns: {}\".format(null_columns))\n",
    "        return null_columns\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the null columns: ['_comments']\n"
     ]
    }
   ],
   "source": [
    "null_columns = check_nulls(results_df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_outliers(null_columns, nulls_expected):\n",
    "    \"\"\"Checks if there are any outlier nulls in the columns\"\"\"\n",
    "    try:\n",
    "        null_outliers=[] #empty list to collect list of columns that are not expected to be null\n",
    "        #if any columns in the nulls expected mismatch the nulls collected, append to null_outliers\n",
    "        if any(x not in nulls_expected for x in null_columns): \n",
    "            null_outliers.append(x)\n",
    "        print(\"These are the outlier null columns: {}\".format(null_outliers))\n",
    "        return null_outliers\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the outlier null columns: []\n"
     ]
    }
   ],
   "source": [
    "null_outliers = check_null_outliers(null_columns, nulls_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out the nullable vs. required mode schema mismatch\n",
    "def upload_to_gbq(results_df_transformed, project_id, dataset_name, table_name):\n",
    "    \"\"\"Uploads data into bigquery and appends if data already exists\"\"\"\n",
    "    try:\n",
    "        gbq.to_gbq(results_df_transformed, dataset_name+\".\"+table_name, project_id, \n",
    "                   if_exists='append', location = 'US', progress_bar=False)\n",
    "        print('Data uploaded into project: {0}, dataset: {1}, table: {2}'.format(project_id, dataset_name, table_name))\n",
    "    except Exception as e:\n",
    "        print(\"Failure to upload data to Bigquery :(\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded into project: iconic-range-220603, dataset: test_sung_1, table: test_sung_5\n"
     ]
    }
   ],
   "source": [
    "upload_to_gbq(results_df_transformed, project_id, dataset_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get record count in raw table\n",
    "# from google.cloud import bigquery\n",
    "# client = bigquery.Client()\n",
    "# dataset_id = 'my_dataset'\n",
    "# table_id = 'my_table'\n",
    "def bq_table_num_rows(dataset_name, table_name):\n",
    "    \"\"\"Print total number of rows in destination bigquery table\"\"\"\n",
    "    try:\n",
    "        bigquery_client = bigquery.Client() #instantiate bigquery client to interact with api\n",
    "        dataset_ref = bigquery_client.dataset(dataset_name) #create dataset object\n",
    "        table_ref = dataset_ref.table(table_name) #create table object \n",
    "        table = bigquery_client.get_table(table_ref)  # API Request\n",
    "        num_rows = table.num_rows\n",
    "        print(\"Total number of rows: {0} in dataset: {1}, table:{2}\".format(table.num_rows, dataset_name, table_name))\n",
    "        return num_rows\n",
    "    except Exception as e:\n",
    "        print(\"Failure to count number of rows in dataset: {0}, table: {1}\".format(dataset_name, table_name))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 5028 in dataset: test_sung_1, table:test_sung_5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5028"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq_table_num_rows(dataset_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query within cloud function to remove duplicates \n",
    "#in another destination dataset/table, incremental load based on date\n",
    "# https://stackoverflow.com/questions/51475252/can-i-use-a-query-parameter-in-a-table-name\n",
    "#order of operations:\n",
    "#1. Get max timestamp from the raw table where timestamp>=today()\n",
    "#2. Select * from raw table > max timestamp and append to unique record table\n",
    "#3. Capture bq total number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#may not be possible to use table parameters in queries\n",
    "#think of something new\n",
    "#what if I define it as a formatted string first and then format it as a docstring using concatenates? It works\n",
    "def query_max_timestamp(project_id, dataset_name, table_name):\n",
    "    \"\"\"Return the max timestamp from a table in BigQuery\"\"\"\n",
    "    #in central Chicago time\n",
    "    sql = \"SELECT max(_last_updt) as max_timestamp FROM `{0}.{1}.{2}` WHERE _last_updt >= TIMESTAMP(CURRENT_DATE('-06:00'))\"\\\n",
    "    .format(project_id, dataset_name, table_name)\n",
    "    \n",
    "    bigquery_client = bigquery.Client() #setup the client\n",
    "    \n",
    "    query_job = bigquery_client.query(sql) #run the query\n",
    "    \n",
    "    results = query_job.result() # waits for job to complete\n",
    "    for row in results:\n",
    "        return row.max_timestamp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 2, 18, 19, 31, 28, tzinfo=<UTC>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_max_timestamp(project_id, dataset_name, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1420437600.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "d = datetime.date(2015,1,5)\n",
    "\n",
    "unixtime = time.mktime(d.timetuple())\n",
    "\n",
    "unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists\n",
      "Created empty table partitioned on column: _last_updt\n"
     ]
    }
   ],
   "source": [
    "#create a table for unique records\n",
    "table_name_2 = 'unique_records'\n",
    "table_desc_2 = \"Unique records from table: {}\".format(table_name)\n",
    "create_dataset_table(dataset_name, table_name_2, table_desc_2, schema_bq, partition_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query unique records and append to unique records table\n",
    "\n",
    "def query_unique_records(project_id, dataset_name, table_name, table_name_2):\n",
    "    \"\"\"Queries unique records from original table and appends to destination table\"\"\"\n",
    "    bigquery_client = bigquery.Client()\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    table_ref = bigquery_client.dataset(dataset_name).table(table_name_2) #set destination table\n",
    "    job_config.destination = table_ref\n",
    "    max_timestamp = query_max_timestamp(project_id, dataset_name, table_name)\n",
    "    sql = \"SELECT * FROM `{0}.{1}.{2}` WHERE _last_updt >= TIMESTAMP(CURRENT_DATE('-06:00'));\"\\\n",
    "    .format(project_id, dataset_name, table_name)\n",
    "    query_job = bigquery_client.query(\n",
    "        sql,\n",
    "        # Location must match that of the dataset(s) referenced in the query\n",
    "        # and of the destination table.\n",
    "        location='US',\n",
    "        job_config=job_config)  # API request - starts the query\n",
    "    query_job.result()\n",
    "    print('Query results loaded to table {}'.format(table_ref.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "Conflict",
     "evalue": "409 Already Exists: Table iconic-range-220603:test_sung_1.unique_records",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConflict\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-460e92a45dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtable_name_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"unique_records\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mquery_unique_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable_name_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-72-3fd1e2ec531c>\u001b[0m in \u001b[0;36mquery_unique_records\u001b[1;34m(project_id, dataset_name, table_name, table_name_2)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlocation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'US'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         job_config=job_config)  # API request - starts the query\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mquery_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Query results loaded to table {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\job.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout, retry)\u001b[0m\n\u001b[0;32m   2760\u001b[0m             \u001b[1;32mnot\u001b[0m \u001b[0mcomplete\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2761\u001b[0m         \"\"\"\n\u001b[1;32m-> 2762\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2763\u001b[0m         \u001b[1;31m# Return an iterator instead of returning the job.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2764\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_query_results\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\job.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout, retry)\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;31m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcancelled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\api_core\\future\\polling.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[1;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConflict\u001b[0m: 409 Already Exists: Table iconic-range-220603:test_sung_1.unique_records"
     ]
    }
   ],
   "source": [
    "table_name_2 = \"unique_records\"\n",
    "query_unique_records(project_id, dataset_name, table_name, table_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "Conflict",
     "evalue": "409 Already Exists: Table iconic-range-220603:test_sung_1.test_table",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConflict\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-1ced6f631c6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     job_config=job_config)  # API request - starts the query\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mquery_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Waits for the query to finish\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Query results loaded to table {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\job.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout, retry)\u001b[0m\n\u001b[0;32m   2760\u001b[0m             \u001b[1;32mnot\u001b[0m \u001b[0mcomplete\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2761\u001b[0m         \"\"\"\n\u001b[1;32m-> 2762\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2763\u001b[0m         \u001b[1;31m# Return an iterator instead of returning the job.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2764\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_query_results\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\job.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout, retry)\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;31m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcancelled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\api_core\\future\\polling.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[1;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConflict\u001b[0m: 409 Already Exists: Table iconic-range-220603:test_sung_1.test_table"
     ]
    }
   ],
   "source": [
    "# from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "dataset_id = dataset_name\n",
    "\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "# Set the destination table\n",
    "table_ref = client.dataset(dataset_id).table('test_table')\n",
    "job_config.destination = table_ref\n",
    "sql = \"\"\"\n",
    "    SELECT corpus\n",
    "    FROM `bigquery-public-data.samples.shakespeare`\n",
    "    GROUP BY corpus;\n",
    "\"\"\"\n",
    "\n",
    "# Start the query, passing in the extra configuration.\n",
    "query_job = client.query(\n",
    "    sql,\n",
    "    # Location must match that of the dataset(s) referenced in the query\n",
    "    # and of the destination table.\n",
    "    location='US',\n",
    "    job_config=job_config)  # API request - starts the query\n",
    "\n",
    "query_job.result()  # Waits for the query to finish\n",
    "print('Query results loaded to table {}'.format(table_ref.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://stackoverflow.com/questions/13530967 : 45146 views\n",
      "https://stackoverflow.com/questions/22879669 : 40825 views\n",
      "https://stackoverflow.com/questions/13221978 : 31773 views\n",
      "https://stackoverflow.com/questions/35159967 : 30384 views\n",
      "https://stackoverflow.com/questions/10604135 : 30055 views\n",
      "https://stackoverflow.com/questions/16609219 : 28891 views\n",
      "https://stackoverflow.com/questions/6607552 : 28681 views\n",
      "https://stackoverflow.com/questions/22004216 : 23714 views\n",
      "https://stackoverflow.com/questions/10644993 : 23551 views\n",
      "https://stackoverflow.com/questions/11647201 : 22790 views\n"
     ]
    },
    {
     "ename": "Forbidden",
     "evalue": "403 POST https://www.googleapis.com/bigquery/v2/projects/your-project-id/jobs: Access Denied: Project your-project-id: The user demo-service-account@iconic-range-220603.iam.gserviceaccount.com does not have bigquery.jobs.create permission in project your-project-id.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-be631908f5e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m# Run a Standard SQL query with the project set explicitly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mproject_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'your-project-id'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\client.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry)\u001b[0m\n\u001b[0;32m   1437\u001b[0m         \u001b[0mjob_ref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_JobReference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m         \u001b[0mquery_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob_ref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjob_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m         \u001b[0mquery_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mquery_job\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\job.py\u001b[0m in \u001b[0;36m_begin\u001b[1;34m(self, client, retry)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[1;31m# job has an ID.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         api_response = client._call_api(\n\u001b[1;32m--> 553\u001b[1;33m             \u001b[0mretry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"POST\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_api_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m         )\n\u001b[0;32m    555\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_properties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapi_response\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\cloud\\bigquery\\client.py\u001b[0m in \u001b[0;36m_call_api\u001b[1;34m(self, retry, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mretry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mretry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_ref\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDEFAULT_RETRY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\api_core\\retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m                 \u001b[0mon_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m             )\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\api_core\\retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\google\\cloud\\_http.py\u001b[0m in \u001b[0;36mapi_request\u001b[1;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mForbidden\u001b[0m: 403 POST https://www.googleapis.com/bigquery/v2/projects/your-project-id/jobs: Access Denied: Project your-project-id: The user demo-service-account@iconic-range-220603.iam.gserviceaccount.com does not have bigquery.jobs.create permission in project your-project-id."
     ]
    }
   ],
   "source": [
    "#SCRATCH WORK\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "def query_stackoverflow():\n",
    "    client = bigquery.Client()\n",
    "    query_job = client.query(\"\"\"\n",
    "        SELECT\n",
    "          CONCAT(\n",
    "            'https://stackoverflow.com/questions/',\n",
    "            CAST(id as STRING)) as url,\n",
    "          view_count\n",
    "        FROM `bigquery-public-data.stackoverflow.posts_questions`\n",
    "        WHERE tags like '%google-bigquery%'\n",
    "        ORDER BY view_count DESC\n",
    "        LIMIT 10\"\"\")\n",
    "\n",
    "    results = query_job.result()  # Waits for job to complete.\n",
    "\n",
    "    for row in results:\n",
    "        print(\"{} : {} views\".format(row.url, row.view_count))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    query_stackoverflow()\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "sql = \"\"\"\n",
    "    SELECT name\n",
    "    FROM `bigquery-public-data.usa_names.usa_1910_current`\n",
    "    WHERE state = 'TX'\n",
    "    LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "# Run a Standard SQL query using the environment's default project\n",
    "df = client.query(sql).to_dataframe()\n",
    "\n",
    "# Run a Standard SQL query with the project set explicitly\n",
    "project_id = 'your-project-id'\n",
    "df = client.query(sql, project=project_id).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return f'Uploaded raw csv file to bucket: {0}, and appended data to BigQuery dataset: {1}, table: {2} on '.format(bucket_name,dataset_name,table_name) + _getToday()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
